# -*- coding: utf-8 -*-
"""mnist_cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WuUIKkSFr9RRK5cOUGbJYdY1HR2ILNh-

# Classification of handwritten digits using MLN

# MNIST Dataset

<img src="https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png" title="MNIST dataset" align="center"/>

# Problem
Classify handwritten digits from 0 - 9. <br>
Each image is 28x28 pixels

<img src="https://corochann.com/wp-content/uploads/2017/02/mnist_plot.png" title="" align="center"/>

# Understanding the data
"""

# MNIST data is present in the keras library. You may load it from there
from keras.datasets import mnist
def load_data():
    """
    Inputs:
        None
    Outputs:
        train_samples, train_labels, test_samples, test_labels: numpy arrays
    
    Load the train/test of mnist data into these variables
    """
    # YOUR CODE HERE
    (train_samples, train_labels), (test_samples, test_labels) = mnist.load_data()
    return train_samples, train_labels, test_samples, test_labels 

train_samples, train_labels, test_samples, test_labels = load_data()

"""Test cases"""
assert train_samples.shape == (60000, 28, 28)
assert test_labels.shape == (10000,)
print('Test passed', '\U0001F44D')

"""### Shape of Data"""

print(train_samples.shape)
print(train_labels.shape)
print(test_samples.shape)
print(test_labels.shape)

"""### Range of Values"""

import numpy as np
np.amax(train_samples) # Max value

np.amin(train_samples) # Min Value

# %matplotlib inline
import matplotlib.pyplot as plt

pixels = train_samples[0] # Shape (28, 28)
plt.imshow(pixels, cmap='gray')
plt.show()
print('Label of image is', train_labels[0])

"""# Data Preparation

### Normalize inputs to (0, 1)
"""

def convert_dtype(x):
    """
    Inputs:
        x: numpy array
    Outputs:
        x_float: numpy array, dtype of elements changed to 'float32'
    """
    # YOUR CODE HERE
    x_float=x.astype('float32')
    return x_float

train_samples = convert_dtype(train_samples)
test_samples = convert_dtype(test_samples)

"""Test cases"""
assert str(train_samples.dtype) == 'float32'
print('Test passed', '\U0001F44D')

def normalize(x):
    """
    Inputs:
        x: numpy array
    Outputs:
        x_n: numpy array, elements normalized to be between (0, 1)
    """    
    # YOUR CODE HERE
    mval=np.max(x)
    x_n=x/mval
    return x_n
    
train_samples = normalize(train_samples)
test_samples = normalize(test_samples)

"""Test cases"""
assert np.isclose(np.amax(train_samples), 1)
print('Test passed', '\U0001F44D')

def reshape(x):
    """
    We need to reshape our train_data to be of shape (samples, height, width, channels) pass to Conv2D layer of keras
    Inputs:
        x: numpy array of shape(samples, height, width)
    Outputs:
        x_r: numpy array of shape(samples, height, width, 1)
    """
    # YOUR CODE HERE
    samples=x.shape[0]
    height=x.shape[1]
    width=x.shape[2]
    x_r=x.reshape(samples, height, width, 1)
    return x_r

train_samples = reshape(train_samples)
test_samples = reshape(test_samples)

train_labels.shape

train_samples.shape

"""### Convert outputs to 1-hot vectors
\begin{equation*}
Eg: 5 \rightarrow [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
\end{equation*}
"""

def oneHot(y, Ny):
    """
    Inputs:
        y: numpy array if shape (samples, ) with class labels
        Ny: number of classes
    Outputs:
        y_oh: numpy array of shape (samples, Ny) of one hot vectors
    """
    # YOUR CODE HERE
    import keras
    y_oh=keras.utils.to_categorical(y, Ny)
    return y_oh

# example
train_labels = oneHot(train_labels, 10)
test_labels = oneHot(test_labels, 10)

"""Test cases"""
assert train_labels.shape[1] == 10
print('Test passed', '\U0001F44D')

"""## Create a convolutional neural network model
You may design whatever cnn model you like. But following are hints to get started.<br>
Make the following layers:
1. cnn layer with kernel_size = (5, 5) and 32 kernels
2. cnn layer with kernel_size = (3, 3) and 20 kernels
3. Maxpooling layer of size (2, 2)
4. Flatten layer
5. Dense layer of appropriate size
6. Output layer of appropriate size
"""

def create_model():
    """
    Inputs:
        None
    Outputs:
        model: compiled keras model
    """
    # YOUR CODE HERE
    from keras.models import Sequential
    from keras.layers import Dense, Dropout, Flatten
    from keras.layers import Conv2D, MaxPooling2D
    model = Sequential()
    model.add(Conv2D(32, kernel_size=(5,5),activation="relu",input_shape=(28,28,1)))
    model.add(Conv2D(64,kernel_size = (5,5),activation="relu"))
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Flatten())
    model.add(Dense(128,activation="relu"))
    model.add(Dense(10,activation="softmax"))
    model.compile(optimizer="adam", loss='categorical_crossentropy', metrics=['accuracy'])
    return model

model = create_model()

model.summary()

history = model.fit(train_samples, train_labels, validation_split = 0.1, epochs=2, batch_size=200)
# Use 10% of samples for validation, validation_split is the relevant parameter

def predict(x):
    """
    Inputs:
        x: input samples
        model: keras model
    Outputs:
        y: predicted labels
    """
    # YOUR CODE HERE
    y=model.predict(x)
    return y

def oneHot_tolabel(y):
    """
    Inputs:
        y: numpy array of shape (samples, Ny)
    Outputs:
        y_b: numpy array of shape (samples,) where one hot encoding is converted back to class labels
    """
    # YOUR CODE HERE
    y_b = np.argmax(y,axis=1)
    return y_b

def create_confusion_matrix(true_labels, predicted_labels):
    """
    Inputs:
        true_labels: numpy array of shape (samples, ) with true_labels
        test_labels: numpy array of shape(samples, ) with test_labels
    Outputs:
        cm: numpy array of shape (Ny, Ny), confusion matrix. Ny -> number of unique classes in y
    """
    # YOUR CODE HERE
    from sklearn.metrics import classification_report, confusion_matrix  
    cm = confusion_matrix(true_labels,predicted_labels)    
    return cm

predicted_labels_test = predict(test_samples)

cm = create_confusion_matrix(oneHot_tolabel(test_labels), oneHot_tolabel(predict(test_samples)))

print(cm)

history.history.keys()
import matplotlib.pyplot as plt
plt.plot(range(len(history.history['val_acc'])), history.history['val_acc'])
plt.show()

def accuracy(x_test, y_test, model):
    """
    Inputs:
        x_test: test samples
        y_test : test labels
        model: keras model
    Ouputs:
        acc: float, accuracy of test data on model
    """
    # YOUR CODE HERE
    from sklearn.metrics import confusion_matrix
    acc = model.evaluate(x_test, y_test,verbose=0)
    return acc

acc = accuracy(test_samples, test_labels, model)
print('Test accuracy is, ', acc[-1]*100, '%')

### Advanced
#1. Tune the hyperparameters to better the performance
#2. Find the classes which are getting most confused from confusion matrix. Take out those samples. Plot them and see why they are getting confused.