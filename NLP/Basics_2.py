# -*- coding: utf-8 -*-
"""multi_hot_sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BgysOMQUPtRFqh_efn_03GKH_DC101F7

# Text classification 
## Sentiment analysis
It is a natural language processing problem where text is understood and the underlying intent is predicted. Here, you need to  predict the sentiment of movie reviews as either positive or negative in Python using the Keras deep learning library.

## Data description
The dataset is the Large Movie Review Dataset often referred to as the IMDB dataset.

The [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/) (often referred to as the IMDB dataset) contains 25,000 highly polar movie reviews (good or bad) for training and the same amount again for testing. The problem is to determine whether a given moving review has a positive or negative sentiment.  Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers).

## Loading dataset
First, we will load complete dataset and analyze some properties of it.<br>
"""

import numpy as np
from matplotlib import pyplot
import numpy
import keras
from keras import regularizers,layers
from keras.datasets import imdb
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence

# np.load is used inside imdb.load_data. But imdb.load_data still assumes the default 
# values of an older version of numpy. So necessary changes to np.load are made

# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)

# load Numpy
np_load_old = np.load

# modify the default parameters of np.load
np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)

# call load_data with allow_pickle implicitly set to true
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)

# restore np.load for future normal usage
np.load = np_load_old

X = np.concatenate((X_train, X_test), axis=0)
y = np.concatenate((y_train, y_test), axis=0)

print(X.shape)
print(X_train.shape)

"""## **Let's see some of reviews.**"""

word_to_id = keras.datasets.imdb.get_word_index()
id_to_word = {value:key for key,value in word_to_id.items()}
for i in range(15,20):
  print("********************************************")
  print(' '.join(id_to_word.get(id - 3, '?')for id in X_train[i] ))

"""## Summarize the data
1) Find out the number of classes in label (*y* array)? <br>
2) Find out number of unique words in dataset *X*?  <br>
3) Calculate the list of review length , report mean and standard deviation. <br>
"""

def summarize_data():
  """
  Output:
                    classes: list, list of unique classes in y  
                no_of_words: int, number of unique words in dataset x 
     list_of_review_lengths: list,  list of lengths of each review 
         mean_review_length: float, mean(list_of_review_lengths), a single floating point value
          std_review_length: float, standard_deviation(list_of_review_lengths), a single floating point value
  """
  # YOUR CODE HERE
  list_of_review_lengths=[]
  classes=np.unique(y)
  unique_words=set()
  for i in X:
    for j in i:
      if j not in unique_words:
        unique_words.add(j)
  no_of_words=len(unique_words)
  
  for i in X:
    list_of_review_lengths.append(len(i))
    
  mean_review_length=np.mean(list_of_review_lengths)
  std_review_length=np.std(list_of_review_lengths)
    
  return classes, no_of_words, list_of_review_lengths, mean_review_length, std_review_length


classes, no_of_words, list_of_review_lengths, mean_review_length, std_review_length = summarize_data()

'''test for summarize_data'''
def test_summarize_data():
  assert classes.tolist() == [0,1]
  assert no_of_words == 9998
  assert np.isclose(mean_review_length, 234.75892, atol = 0.001)
  assert np.isclose(std_review_length, 172.91149458735703, atol = 0.001)
  print('Test passed', '\U0001F44D')
test_summarize_data()

classes

y

y_train

"""## One hot encode the output data"""

def one_hot(y):
  """
  Inputs:
    y: numpy array with class labels
  Outputs:
    y_oh: numpy array with corresponding one-hot encodings
  """
  # YOUR CODE HERE
  y_oh=[]
  for i in range(len(y)):
    if y[i] == 0:
      y_oh.append([1,0])
    else:
       y_oh.append([0,1])
  y_oh=np.array(y_oh)
    
  
   
  return y_oh

  
y_train = one_hot(y_train)
y_test = one_hot(y_test)

y_train

"""### Multi-hot encode the input data

All sequences are of different length and our vocabulory size is 10K.  <br>
**To Do**<br>
1) Intialize vector of dimension 10,000 with value 0. <br>
2) For those tokens in a sequence which are present in Vocabulary make that position as 1 and keep all other positions filled with 0. <br>
For example, lets take Vocabulary = ['I': 0, ':1, 'eat: 2:' mango: 3, 'fruit':4, 'happy':5, 'you':6] <br>
We have two sequnces and 
Multi-hot encoding of both sequences will be of dimension:  7 (vocab size).<br>
1) *Mango is my favourite fruit* becomes *Mango ? ? ? fruit* after removing words which are not in my vocabulary. Hence multi hot encoding will have two 1's corresponding to mango and fruit i.e, [0, 0, 0, 1, 1, 0, 0] <br>
Similarly, <br>
  2) *I love to eat mango*  = *I ? ? eat mango*  =  [1, 1, 0, 1, 0, 0, 0]
"""

def multi_hot_encode(sequences, dimension):
  """
    Input:
          sequences: list of sequences in X_train or X_test

    Output:
          results: mult numpy matrix of shape(len(sequences), dimension)
                  
  """
  # YOUR CODE HERE
  results=np.zeros((len(sequences),dimension))
  count = 0
  for i in sequences:
    for j in i:
      results[count,j-1] = 1
    count = count + 1
  
        
  return results

X_train

x_train = multi_hot_encode(X_train, 10000)
x_test = multi_hot_encode(X_test, 10000)

print("x_train ", x_train.shape)
print("x_test ", x_test.shape)

x_train

'''test for pad_sequences'''
def test_multi_hot_encode():
  assert np.sum(x_train[1]) == 121.0
  print('Test passed', '\U0001F44D')
test_multi_hot_encode()

"""## Split the data into train and validation"""

from sklearn.model_selection import train_test_split
x_strat, x_dev, y_strat, y_dev = train_test_split(x_train, y_train,test_size=0.40,random_state=0, stratify=y_train)

"""## Build Model
Build a multi layered feed forward network in keras.

### Create the model
"""

def create_model():
    """
    Output:
        model: A compiled keras model
    """
    # YOUR CODE HERE
    import keras
    from keras.layers import Input, Dense
    from keras.models import Model
    from keras import optimizers
    
    x= Input(x_train.shape[1:])
    h=Dense(10,)(x)
    y=Dense(len(classes),activation='softmax')(h)
    model=Model(inputs=x, outputs=y)
    model.compile(optimizer=optimizers.sgd(lr=0.01),loss='categorical_crossentropy',metrics=['accuracy'])
    model.summary()
    return model
  
model = create_model()
print(model.summary())

"""### Fit the Model"""

import matplotlib.pyplot as plt
def fit(model):
    """
    Action:
        Fit the model created above using training data as x_strat and y_strat
        and validation_data as x_dev and y_dev, verbose=2 and store it in 'history' variable.
        
        evaluate the model using x_test, y_test, verbose=0 and store it in 'scores' list
    Output:
        scores: list of length 2
        history_dict: output of history.history where history is output of model.fit()
    """
    # YOUR CODE HERE
    history=model.fit(x_strat, y_strat,epochs=5,batch_size=2, verbose=2, validation_data=(x_dev,y_dev))
    scores = model.predict(x_test)
    history_dict = history.history
    
    return scores,history_dict
    
scores,history_dict = fit(model)

scores

Accuracy=scores[1]*100
print('Accuracy of your model is')
print(scores[1]*100)

history_dict['loss']

"""### Verify whether training in converged or not"""

import matplotlib.pyplot as plt
plt.clf()
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
epochs = range(1, (len(history_dict['loss']) + 1))
plt.plot(epochs, loss_values, 'bo', label='Training loss')
plt.plot(epochs, val_loss_values, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.clf()
acc_values = history_dict['acc']
val_acc_values = history_dict['val_acc']
epochs = range(1, (len(history_dict['acc']) + 1))
plt.plot(epochs, acc_values, 'bo', label='Training acc')
plt.plot(epochs, val_acc_values, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""### Advanced
1. Find some reviews where your model fails to predict the sentiment correctly and give the reason why.
2. Write 5 reviews on your own with at least 20 words. See if your model correctly predicts the sentiment on these reviews
"""

